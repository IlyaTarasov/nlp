{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a650a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install natasha\n",
    "!pip install spacy\n",
    "!python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d80455",
   "metadata": {},
   "source": [
    "### Пункт1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc408e",
   "metadata": {},
   "source": [
    "Я взял отрывок стихотворения из романа Набокова. Во-первых, я хочу проверить насколько хорошо теггеры справляются с различением имен собственных и просто слов, написанных с большой буквы. Во-вторых, Набоков обильно поставляет сложные слова, редкие слова, окказионализмы. Также тут много местоимений, причастий, деепричастий, частиц, союзов, которые могут представлять сложности с точки зрения выбора подхода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37cf67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Я тень, я свиристель, убитый влет\n",
    "Подложной синью, взятой в переплет\n",
    "Окна; комочек пепла, легкий прах,\n",
    "Порхнувший в отраженных небесах.\n",
    "Так и снутри удвоены во мне\n",
    "Я сам, тарелка, яблоко на ней;\n",
    "Раздвинув ночью шторы, за стеклом\n",
    "Я открываю кресло со столом,\n",
    "Висящие над темной гладью сада,\n",
    "\n",
    "Но лучше, если после снегопада\n",
    "Они, как на ковре, стоят вовне --\n",
    "Там, на снегу, в хрустальнейшей стране!\n",
    "\n",
    "Вернемся в снегопад: здесь каждый клок\n",
    "Бесформен, медлен, вял и одинок.\n",
    "Унылый мрак, белесый бледный день,\n",
    "Нейтральный свет, абстрактных сосен сень.\n",
    "В ограду сини вкрадчиво-скользящей\n",
    "Ночь заключит картину со смотрящим;\n",
    "А утром -- чьи пришпоренные ноги\n",
    "\n",
    "Вписали строчку в чистый лист дороги? --\n",
    "Дивится перл мороза. Снова мы\n",
    "Направо слева ясный шифр зимы\n",
    "Читаем: точка, стрелка вспять, штришок,\n",
    "Вновь точка, стрелка вспять... фазаний скок!\n",
    "Се гордый граус, родственник тетерки\n",
    "Китаем наши претворил задворки.\n",
    "Из \"Хольмса\", что ли: вспять уводит след,\n",
    "Когда башмак назад носком надет.\n",
    "\n",
    "Был люб мне, взоры грея, всякий цвет.\n",
    "\n",
    "Я мог сфотографировать предмет\n",
    "В своем зрачке. Довольно было мне\n",
    "Глазам дать волю или, в тишине,\n",
    "Шепнуть приказ, -- и все, что видит взор, --\n",
    "Паркет, гикори лиственный убор,\n",
    "Застрех, капели стылые стилеты\n",
    "На дне глазницы оседало где-то\n",
    "И сохранялось час, и два. Пока\n",
    "Все это длилось, стоило слегка\n",
    "Прикрыть глаза -- и заново узришь\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca0c5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "#я вырезал знаки препинания для удобства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c2ce3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "tokens[88] = 'вкрадчиво-скользящей'\n",
    "tokens[189] = 'где-то'\n",
    "#вернул дефисы словам, потерявшим его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c45c30ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('corpora.csv', \"w\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames = [\"tokens\", \"POS\"], delimiter = ',')\n",
    "    writer.writeheader()\n",
    "    for token in tokens:\n",
    "        f.write(token + '\\n')\n",
    "#записываю в csv табличку для удобства размечания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b308377e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = open('corpora.csv', 'r')\n",
    "file = csv.DictReader(filename)\n",
    "pos_gold = []\n",
    "for col in file:\n",
    "    pos_gold.append(col['POS'])\n",
    "#достаю список с размеченными тегами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d21c33",
   "metadata": {},
   "source": [
    "### Пункт2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b3a6a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "pos_pymorphy = []\n",
    "\n",
    "for token in tokens:\n",
    "    ana = morph.parse(token)\n",
    "    first = ana[0]\n",
    "    pos_pymorphy.append(first.tag.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f7c49b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "\n",
    "morph_tagger = NewsMorphTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e3fae330",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_natasha = []\n",
    "\n",
    "doc = Doc(' '.join(tokens))\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "for token in doc.tokens:\n",
    "    pos_natasha.append(token.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "73f33efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "pos_spacy = []\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "text = ' '.join(tokens)\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    #spacy не самым приятным способом работает с дефисом, поэтому вручную делаю, чтобы не было 3 сущностей на 1 токен\n",
    "    if token.text not in ['-', 'то', 'вкрадчиво']:\n",
    "        pos_spacy.append(token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b733e78",
   "metadata": {},
   "source": [
    "### Пункт3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620f01e",
   "metadata": {},
   "source": [
    "Я сам делал разметку в соответствии с принципами UD, я уже работал с ним раньше, он кажется мне простым и удобным, только все PART отнес к ADV, мне кажется так сильно проще, а разделение не принципиально. Поэтому я привожу PART к ADV для наташи и стэйси, т.к. они тоже основаны на UD, а для пайморфи проделываю более основательную переработку. Также я решил объединить союзы в conj, т.к. выудить разделение из pymorphy не представляется возможным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "57de8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pos_spacy)):\n",
    "    if pos_spacy[i] == 'PART':\n",
    "        pos_spacy[i] = 'ADV'\n",
    "    if pos_spacy[i] in ['SCONJ', 'CCONJ']:\n",
    "        pos_spacy[i] = 'CONJ'\n",
    "    if pos_natasha[i] == 'PART':\n",
    "        pos_natasha[i] = 'ADV'\n",
    "    if pos_natasha[i] in ['SCONJ', 'CCONJ']:\n",
    "        pos_natasha[i] = 'CONJ'\n",
    "    if pos_pymorphy[i] in ['ADVB', 'PRCL']:\n",
    "        pos_pymorphy[i] = 'ADV'\n",
    "    if pos_pymorphy[i] in ['INFN', 'GRND', 'PRTF', 'PRTS']:\n",
    "        pos_pymorphy[i] = 'VERB'\n",
    "    if pos_pymorphy[i] == 'NUMR':\n",
    "        pos_pymorphy[i] = 'NUM'\n",
    "    if pos_pymorphy[i] == 'PREP':\n",
    "        pos_pymorphy[i] = 'ADP'\n",
    "    if pos_pymorphy[i] in ['ADJF', 'ADJS']:\n",
    "        pos_pymorphy[i] = 'ADJ'\n",
    "    if pos_pymorphy[i] == 'NPRO':\n",
    "        pos_pymorphy[i] = 'PRON'\n",
    "# COMP нет желания как-то адаптировать, просто неадекватный подход"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "405d35f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_pypormphy: 0.8009708737864077\n",
      "accuracy_natasha: 0.7330097087378641\n",
      "accuracy_spacy: 0.7912621359223301\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy_pypormphy:', accuracy_score(pos_gold, pos_pymorphy))\n",
    "print('accuracy_natasha:', accuracy_score(pos_gold, pos_natasha))\n",
    "print('accuracy_spacy:', accuracy_score(pos_gold, pos_spacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb2170",
   "metadata": {},
   "source": [
    "На удивление лучший результат показывает pymorphy (это при том, что мы раскрывали омонимию по первому варианту), хотя, предварительно разглядывая, я ожидал в победителях spacy, возможно, некоторое упрощение его(пайморфи) более разветвленной разметки сыграло на руку, но изначально большая точность как будто заслуживает такого исхода. Ключевую разницу сыграли здесь слова с большой буквы: наташа постоянно пыталась отнести их к PROPN, поэтому ее результат заметно хуже. Точнее всего с ними работал как раз pymormphy, также часто pymorphy обильно использовал ADV, иногда попадая точнее других моделей, иногда, напротив, ошибаясь. Мне кажется релевантным выделение DET, и, хотя pymorphy не использовала этот тег, это ей не помешало занять первое место (если пойти навстречу ей в этом вопрсое точность окажется очень высокой). Выполним следующий шанс с pymorphy. Учитывая сложность текста, кажется, все модели справились достойно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca39d7",
   "metadata": {},
   "source": [
    "### Пункт4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43202080",
   "metadata": {},
   "source": [
    "я хочу взять следующие 3 шаблона: ADJ + игра, ADJ + баланс, ADV + играть. Мне кажется, что биграммы таких шаблонов частотны в отзывах всех типов, при этом достаточно однозначно характеризуют отношение к игре, отдельно же данные прилагательные могут встречаться в отзывах обеих тональностей, в результате чего, скорее всего, самоуничтожаются, отдельно же существительные так же самоуничтожаются и не несут какого-либо положительного или отрицательного отношения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec7531",
   "metadata": {},
   "source": [
    "Возьмем часть кода, в которой я записывал слова по словарям и встроим в нее чанкер. Изначально она выглядит так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03472236",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    if y_train[i] == 'pos':\n",
    "        for token in X_train[i]:\n",
    "            if token not in dict_pos.keys():\n",
    "                dict_pos[token] = 1\n",
    "            else:\n",
    "                dict_pos[token] += 1\n",
    "    else:\n",
    "        for token in X_train[i]:\n",
    "            if token not in dict_neg.keys():\n",
    "                dict_neg[token] = 1\n",
    "            else:\n",
    "                dict_neg[token] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cec1fb",
   "metadata": {},
   "source": [
    "После интеграции чанкера получаем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f433015",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    if y_train[i] == 'pos':\n",
    "        for x, token in enumerate(X_train[i]):\n",
    "            if token not in dict_pos.keys():\n",
    "                dict_pos[token] = 1\n",
    "            else:\n",
    "                dict_pos[token] += 1\n",
    "            if token in ['игра', 'баланс'] and x != 0:\n",
    "                #смотрим на предыдущее слово, вдруг оно ADJ\n",
    "                ana = morph.parse(X_train[i][x-1])\n",
    "                first = ana[0]\n",
    "                if first.tag.POS == 'ADJF':\n",
    "                    bigram = X_train[i][x-1] + ' ' + X_train[i][x]\n",
    "                    if bigram not in dict_pos.keys():\n",
    "                        dict_pos[bigram] = 1\n",
    "                    else:\n",
    "                        dict_pos[bigram] += 1\n",
    "            elif token == 'играть' and x != 0:\n",
    "                #смотрим на предыдущее слово, вдруг оно ADV\n",
    "                ana = morph.parse(X_train[i][x-1])\n",
    "                first = ana[0]\n",
    "                if first.tag.POS == 'ADVB':\n",
    "                    bigram = X_train[i][x-1] + ' ' + X_train[i][x]\n",
    "                    if bigram not in dict_pos.keys():\n",
    "                        dict_pos[bigram] = 1\n",
    "                    else:\n",
    "                        dict_pos[bigram] += 1\n",
    "                \n",
    "    else:\n",
    "        for x, token in enumerate(X_train[i]):\n",
    "            if token not in dict_neg.keys():\n",
    "                dict_neg[token] = 1\n",
    "            else:\n",
    "                dict_neg[token] += 1\n",
    "            if token in ['игра', 'баланс'] and x != 0:\n",
    "                #смотрим на предыдущее слово, вдруг оно ADJ\n",
    "                ana = morph.parse(X_train[i][x-1])\n",
    "                first = ana[0]\n",
    "                if first.tag.POS == 'ADJF':\n",
    "                    bigram = X_train[i][x-1] + ' ' + X_train[i][x]\n",
    "                    if bigram not in dict_neg.keys():\n",
    "                        dict_neg[bigram] = 1\n",
    "                    else:\n",
    "                        dict_neg[bigram] += 1\n",
    "            elif token == 'играть' and x != 0:\n",
    "                #смотрим на предыдущее слово, вдруг оно ADV\n",
    "                ana = morph.parse(X_train[i][x-1])\n",
    "                first = ana[0]\n",
    "                if first.tag.POS == 'ADVB':\n",
    "                    bigram = X_train[i][x-1] + ' ' + X_train[i][x]\n",
    "                    if bigram not in dict_neg.keys():\n",
    "                        dict_neg[bigram] = 1\n",
    "                    else:\n",
    "                        dict_neg[bigram] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1a932",
   "metadata": {},
   "source": [
    "на деле точность не меняется. список положительных слов полезно пополнился, а вот список негативных не пополняется по старой проблеме: среди положительных отзывов много на самом деле отрицательных отзывов с хорошей оценкой. вероятно, на большей выборке результат появится, если среди положительных отзывов не будет негативных текстов, то он значительно улучшится. Я убежден, что теги валидно подобраны. Хуже результат от них точно не станет. Еще, вероятно, биграммам сложно пробиться через барьер >2/1 употребления на таком маленьком количестве текстов"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
